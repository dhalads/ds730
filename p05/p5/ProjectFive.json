{"paragraphs":[{"text":"//Problem 1\n\n/*\n(5 pts) Not including the header row, how many rows are there in the file?\n*/\n\n/*\nVendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount\n2,11/21/2018 07:20:05 PM,11/21/2018 07:21:49 PM,1,0.38,1,N,142,142,1,3.5,1,0.5,1.06,0,0.3,6.36\n2,11/21/2018 07:17:42 PM,11/21/2018 07:24:37 PM,1,1.38,1,N,166,151,2,7,1,0.5,0,0,0.3,8.8\n1,11/21/2018 07:07:19 PM,11/21/2018 07:34:31 PM,1,5,1,N,136,182,2,20,1,0.5,0,0,0.3,21.8\n1,11/21/2018 07:12:45 PM,11/21/2018 07:16:51 PM,1,0.8,1,N,158,90,2,5,1,0.5,0,0,0.3,6.8\n1,11/21/2018 07:17:59 PM,11/21/2018 07:29:29 PM,0,1.4,1,N,48,142,2,9,1,0.5,0,0,0.3,10.8\n*/\n\nval taxi = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\",true).load(\"/user/zeppelin/taxi/taxi2018.csv\")\ntaxi.count()","user":"anonymous","dateUpdated":"2020-12-01T02:15:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\nres669: Long = 55918\n"}]},"apps":[],"jobName":"paragraph_1606735063436_-535791452","id":"20201130-111743_806811519","dateCreated":"2020-11-30T11:17:43+0000","dateStarted":"2020-12-01T02:15:29+0000","dateFinished":"2020-12-01T02:15:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5645"},{"text":"//Problem 2\n\n/*\n(10 pts) One might posit that a single passenger is the most common taxi ride.\nWhich was more common: a single passenger_count or a passenger_count with\nmore than 1 person? Your code should output the total number of rows with a\npassenger_count of 1 and the total number of rows with a passenger_count\ngreater than 1.\n*/\n\nval taxi = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\",true).load(\"/user/zeppelin/taxi/taxi2018.csv\").select($\"passenger_count\")\n\nval withPassengerGroup = taxi.filter(col(\"passenger_count\")>0).withColumn(\"passengerGroup\", \n    when(col(\"passenger_count\") === 1, \"one\")\n    .when(col(\"passenger_count\") > 1, \"more than one\")\n    .when(col(\"passenger_count\") === 0 , \"zero\")\n    .otherwise(\"none\")\n    )\n\nwithPassengerGroup.createOrReplaceTempView(\"taxiView\")\n\nval output = spark.sqlContext.sql(\"SELECT passengerGroup, COUNT(*) AS tripCount FROM taxiView GROUP BY passengerGroup\")\noutput.show(false)\n","user":"anonymous","dateUpdated":"2020-12-01T02:15:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi: org.apache.spark.sql.DataFrame = [passenger_count: int]\nwithPassengerGroup: org.apache.spark.sql.DataFrame = [passenger_count: int, passengerGroup: string]\noutput: org.apache.spark.sql.DataFrame = [passengerGroup: string, tripCount: bigint]\n+--------------+---------+\n|passengerGroup|tripCount|\n+--------------+---------+\n|one           |39670    |\n|more than one |15752    |\n+--------------+---------+\n\n"}]},"apps":[],"jobName":"paragraph_1606735100070_1210629778","id":"20201130-111820_523844436","dateCreated":"2020-11-30T11:18:20+0000","dateStarted":"2020-12-01T02:15:29+0000","dateFinished":"2020-12-01T02:15:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5646"},{"text":"//Problem 3\n\n/*\n(15 pts) It is possible that people who paid a toll went further than people who\ndidn’t pay a toll. What is the average trip_distance for people who had\ntolls_amount greater than 0? What is the average trip_distance for people who\npaid nothing in tolls?\n*/\n\n\nval taxi = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\",true).load(\"/user/zeppelin/taxi/taxi2018.csv\").select($\"trip_distance\", $\"tolls_amount\")\n\nval withTollsPaid = taxi.filter(col(\"passenger_count\")>0).withColumn(\"tollsPaid\", \n    when(col(\"tolls_amount\") > 0, \"Yes\")\n    .otherwise(\"No\")\n    )\n\nwithTollsPaid.createOrReplaceTempView(\"taxiView\")\n\nval output = spark.sqlContext.sql(\"SELECT tollsPaid, AVG(trip_distance) AS avgDistance FROM taxiView GROUP BY tollsPaid\")\noutput.show(false)\n","user":"anonymous","dateUpdated":"2020-12-01T02:15:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi: org.apache.spark.sql.DataFrame = [trip_distance: double, tolls_amount: double]\nwithTollsPaid: org.apache.spark.sql.DataFrame = [trip_distance: double, tolls_amount: double ... 1 more field]\noutput: org.apache.spark.sql.DataFrame = [tollsPaid: string, avgDistance: double]\n+---------+------------------+\n|tollsPaid|avgDistance       |\n+---------+------------------+\n|No       |2.3442352132312054|\n|Yes      |13.027572688663833|\n+---------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1606735148686_2015940398","id":"20201130-111908_521635750","dateCreated":"2020-11-30T11:19:08+0000","dateStarted":"2020-12-01T02:15:31+0000","dateFinished":"2020-12-01T02:15:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5647"},{"text":"//Problem 4\n\n/*\n(20 pts) Which month has the highest average fare_amount? You should use the\npickup date/time as the month to which a row belongs. You should take the sum\nof the fare_amounts and divide it by the total number of rows for that month. To\nensure we have reliable data, you should filter out all rows where the\nfare_amount is less than or equal to 0. You should filter out all rows where the\n(fare_amount / trip_distance) is greater than 10,000. An obvious side-effect of\nthis is to filter out all rows with a trip_distance of 0. Your code should print out the\naverage fare_amount for each month.\n*/\n\nval taxi = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\",true).load(\"/user/zeppelin/taxi/taxi2018.csv\").select($\"tpep_pickup_datetime\", $\"trip_distance\", $\"fare_amount\")\n\nval withExtra = taxi.filter(col(\"trip_distance\")>0 && col(\"fare_amount\")>0).filter((col(\"fare_amount\")/col(\"trip_distance\"))<10000).withColumn(\"month\", \n    month(to_timestamp($\"tpep_pickup_datetime\", \"MM/dd/yyyy hh:mm:ss a\"))\n   )\n\nwithExtra.createOrReplaceTempView(\"taxiView\")\n\nval output = spark.sqlContext.sql(\"SELECT month, AVG(fare_amount) AS avgFare FROM taxiView GROUP BY month ORDER BY AVG(fare_amount) DESC\")\noutput.show(false)\n","user":"anonymous","dateUpdated":"2020-12-01T02:15:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi: org.apache.spark.sql.DataFrame = [tpep_pickup_datetime: string, trip_distance: double ... 1 more field]\nwithExtra: org.apache.spark.sql.DataFrame = [tpep_pickup_datetime: string, trip_distance: double ... 2 more fields]\noutput: org.apache.spark.sql.DataFrame = [month: int, avgFare: double]\n+-----+------------------+\n|month|avgFare           |\n+-----+------------------+\n|5    |13.595211546936978|\n|10   |13.511325791855203|\n|8    |13.353933418693982|\n|7    |13.326148683531233|\n|12   |13.234322371699053|\n|11   |13.231794294294295|\n|4    |12.936855154414964|\n|6    |12.845854004252303|\n|9    |12.652934236947791|\n|3    |12.528635446532867|\n|1    |12.381402298850574|\n|2    |12.148956584312785|\n+-----+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1606735186191_-2116400707","id":"20201130-111946_1483275412","dateCreated":"2020-11-30T11:19:46+0000","dateStarted":"2020-12-01T02:15:34+0000","dateFinished":"2020-12-01T02:15:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5648"},{"text":"//Problem 5\n\n/*\n(15 pts) Who tips better: people who pay with a credit card before noon or people\nwho pay with credit card from noon until the end of the day (i.e. 12:00:00pm -\n11:59:59pm)? We will use the dropoff time to determine when a person paid. The\npayment_type has a numeric score of 1 if the person paid by a credit card. To\nfigure out who tipped the best, take the sum of the tip_amount and divide it by\nthe sum of the fare_amount. You should print out the (sum(tip_amount) /\nsum(fare_amount)) for each of the 2 requested groups.\n*/\n\n\nval taxi = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\",true).load(\"/user/zeppelin/taxi/taxi2018.csv\").select($\"tpep_dropoff_datetime\", $\"payment_type\", $\"fare_amount\", $\"tip_amount\")\n\nval withExtra = taxi.filter(col(\"payment_type\")===1).withColumn(\"AM_or_PM\", \n     from_unixtime(unix_timestamp($\"tpep_dropoff_datetime\", \"MM/dd/yyyy hh:mm:ss a\"), \"a\")\n   )\n\nwithExtra.createOrReplaceTempView(\"taxiView\")\n\nval output = spark.sqlContext.sql(\"SELECT AM_or_PM, (sum(tip_amount)/sum(fare_amount)) AS ratioTipToFare FROM taxiView GROUP BY AM_or_PM\")\noutput.show(false)\n","user":"anonymous","dateUpdated":"2020-12-01T02:15:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi: org.apache.spark.sql.DataFrame = [tpep_dropoff_datetime: string, payment_type: int ... 2 more fields]\nwithExtra: org.apache.spark.sql.DataFrame = [tpep_dropoff_datetime: string, payment_type: int ... 3 more fields]\noutput: org.apache.spark.sql.DataFrame = [AM_or_PM: string, ratioTipToFare: double]\n+--------+-------------------+\n|AM_or_PM|ratioTipToFare     |\n+--------+-------------------+\n|PM      |0.20209688769666953|\n|AM      |0.1968107145335785 |\n+--------+-------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1606735247263_732957702","id":"20201130-112047_1595025681","dateCreated":"2020-11-30T11:20:47+0000","dateStarted":"2020-12-01T02:15:36+0000","dateFinished":"2020-12-01T02:15:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5649"},{"text":"//Problem 6\n\n/*\n(15 pts) What are the top ten worst rides with respect to time and distance? In\nother words, who sat in the taxi the longest and went the shortest distance? You\nshould remove any ride whose trip_distance is 0. We want to maximize the\nfollowing calculation:\n(number of seconds in the taxi / trip_distance)\nYour code should print out the entire row along with the new column representing\nthe calculation in descending order. You only need to print out 10 answers and\ndo not need to worry about ranks.\n*/\n\nval taxi = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\",true).load(\"/user/zeppelin/taxi/taxi2018.csv\")\n\nval withExtra = taxi.filter(col(\"trip_distance\")>0).withColumn(\n    \"durationPerDistance\", \n    (unix_timestamp($\"tpep_dropoff_datetime\", \"MM/dd/yyyy hh:mm:ss a\") - unix_timestamp($\"tpep_pickup_datetime\", \"MM/dd/yyyy hh:mm:ss a\"))/col(\"trip_distance\")\n    )\n\nwithExtra.createOrReplaceTempView(\"taxiView\")\n\nval output = spark.sqlContext.sql(\"SELECT * FROM taxiView ORDER BY durationPerDistance DESC LIMIT 10\")\noutput.show(false)\n","user":"anonymous","dateUpdated":"2020-12-01T02:15:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\nwithExtra: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 16 more fields]\noutput: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 16 more fields]\n+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+\n|VendorID|tpep_pickup_datetime  |tpep_dropoff_datetime |passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|durationPerDistance|\n+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+\n|2       |02/06/2018 02:28:03 PM|02/07/2018 02:21:24 PM|2              |0.29         |1         |N                 |230         |164         |2           |4.0        |0.0  |0.5    |0.0       |0.0         |0.3                  |4.8         |296555.1724137931  |\n|2       |04/16/2018 07:33:48 PM|04/17/2018 06:45:27 PM|1              |0.45         |1         |N                 |181         |181         |2           |4.0        |1.0  |0.5    |0.0       |0.0         |0.3                  |5.8         |185553.33333333334 |\n|2       |03/15/2018 09:47:38 PM|03/16/2018 09:45:34 PM|1              |0.47         |1         |N                 |75          |75          |2           |4.5        |0.5  |0.5    |0.0       |0.0         |0.3                  |5.8         |183565.95744680852 |\n|2       |06/08/2018 12:23:36 AM|06/08/2018 02:34:34 AM|1              |0.05         |2         |N                 |132         |142         |1           |52.0       |0.0  |0.5    |15.84     |0.0         |0.3                  |68.64       |157160.0           |\n|2       |01/20/2018 05:37:12 PM|01/21/2018 04:38:53 PM|1              |0.63         |1         |N                 |48          |142         |1           |4.0        |0.0  |0.5    |0.96      |0.0         |0.3                  |5.76        |131588.88888888888 |\n|2       |11/27/2018 06:48:08 PM|11/28/2018 06:27:02 PM|5              |0.69         |1         |N                 |262         |263         |2           |5.0        |1.0  |0.5    |0.0       |0.0         |0.3                  |6.8         |123382.60869565219 |\n|2       |03/27/2018 02:11:46 AM|03/28/2018 02:09:13 AM|1              |0.79         |1         |N                 |170         |186         |2           |5.5        |0.5  |0.5    |0.0       |0.0         |0.3                  |6.8         |109173.41772151898 |\n|2       |03/07/2018 06:28:02 PM|03/08/2018 05:32:58 PM|1              |0.77         |1         |N                 |143         |48          |1           |5.0        |1.0  |0.5    |1.02      |0.0         |0.3                  |7.82        |107916.88311688311 |\n|2       |11/02/2018 05:49:35 PM|11/03/2018 05:43:20 PM|1              |0.82         |1         |N                 |161         |233         |2           |8.5        |1.0  |0.5    |0.0       |0.0         |0.3                  |10.3        |104908.53658536586 |\n|2       |06/06/2018 12:51:19 PM|06/07/2018 12:49:44 PM|1              |0.83         |1         |N                 |170         |161         |1           |12.5       |0.0  |0.5    |2.66      |0.0         |0.3                  |15.96       |103981.92771084337 |\n+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1606735316130_-169739838","id":"20201130-112156_1874688382","dateCreated":"2020-11-30T11:21:56+0000","dateStarted":"2020-12-01T02:15:40+0000","dateFinished":"2020-12-01T02:15:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5650"},{"text":"//Problem 7\n\n/*\n(30 pts) Imagine that you work in NYC. You are considering working for a\nride-sharing company to make a few extra dollars later in the evening. However,\nyou don't have that much time to spend driving other people around. You realize\nyou have 60 minutes that you can drive between 4:00:00pm and 11:00:00pm1.\nYou have determined that any total_amount that is over $200 is not going to be\npossible so those ought to be filtered out. You want to maximize the amount of\nmoney you could earn so you want to find the best 60 minute period between\n4:00:00pm and 11:00:00pm, inclusive, to drive. You do not care about days. In\nother words, a ride that starts at 9:45:13pm on June 12th is in the same 60\nminute timeslot as a ride that starts at 9:45:13pm on August 15th.\nYour goal is this, find the 60 minute time slot where you maximize the average\n(mean) total_amount. You are only considering rides starting (i.e.\ntpep_pickup_datetime) between 4:00:00pm and 11:00:00pm (you do not care\nwhen those rides finish) and only rides that are $200 or less. Your answer should\ngo down to the second. In other words, your answer should be something similar\nto 9:47:15pm - 10:47:14pm2. Your entire 60 minute time slot must be between\n4:00:00pm and 11:00:00pm, again, inclusive. Your answer should not go outside\nof these boundaries. Be sure that your code outputs your answer as a full 60\nminute timeslot like this: 6:36:29pm - 7:36:28pm.\n*/\n\nval taxi = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\",true).load(\"/user/zeppelin/taxi/taxi2018.csv\").select($\"tpep_pickup_datetime\", $\"total_amount\")\n\nval withExtra = taxi.filter(col(\"total_amount\")<=200).withColumn(\"pickup_timestamp\", \n    to_timestamp($\"tpep_pickup_datetime\", \"MM/dd/yyyy hh:mm:ss a\")\n    ).withColumn(\"pickup_hour\", \n    hour(col(\"pickup_timestamp\"))\n    ).withColumn(\"pickup_minute\", \n    minute(col(\"pickup_timestamp\"))\n    ).withColumn(\"pickup_second\", \n    second(col(\"pickup_timestamp\"))\n    ).withColumn(\"totalSeconds\",\n    col(\"pickup_hour\")*3600 +col(\"pickup_minute\")*60 + col(\"pickup_second\")\n    ).filter(col(\"totalSeconds\")>=16*3600+0*60+0\n    )\n \nwithExtra.printSchema()\n// withExtra.show(false)\n\n\nimport org.apache.spark.sql.expressions._\n\nval windowSpec = Window.orderBy(\"totalSeconds\").rangeBetween(0,3600)\nval windowSpec2 = Window.orderBy($\"mean_total\".desc)\n\n\nval withExtra2 = withExtra.withColumn(\"mean_total\", avg(withExtra(\"total_amount\")).over(windowSpec)).filter(col(\"totalSeconds\")<=23*3600+0*60+0\n    ).withColumn(\"dense_rank\", rank().over(windowSpec2))\n    \n// withExtra2.show(false)\n\n// withExtra2.coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").option(\"sep\",\",\").save(\"/user/zeppelin/p5_output\")\n\nval answer = withExtra2.filter($\"dense_rank\" <= 1).withColumn(\"window_start\", \n    from_unixtime($\"totalSeconds\", \"hh:mm:ss a\")\n    ).withColumn(\"window_end\", \n    from_unixtime($\"totalSeconds\"+3600, \"hh:mm:ss a\")\n    ).withColumn(\"timeslot\", \n    concat(col(\"window_start\"),lit(\" - \"),col(\"window_end\"))\n    )\n\n// answer.show(false)\n\nval answer2 = answer.select(\"timeslot\").distinct\n\nanswer2.show(false)\n\n\n","user":"anonymous","dateUpdated":"2020-12-01T02:15:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi: org.apache.spark.sql.DataFrame = [tpep_pickup_datetime: string, total_amount: double]\nwithExtra: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [tpep_pickup_datetime: string, total_amount: double ... 5 more fields]\nroot\n |-- tpep_pickup_datetime: string (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- pickup_timestamp: timestamp (nullable = true)\n |-- pickup_hour: integer (nullable = true)\n |-- pickup_minute: integer (nullable = true)\n |-- pickup_second: integer (nullable = true)\n |-- totalSeconds: integer (nullable = true)\n\nimport org.apache.spark.sql.expressions._\nwindowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5d1f0a54\nwindowSpec2: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5dd7535f\nwithExtra2: org.apache.spark.sql.DataFrame = [tpep_pickup_datetime: string, total_amount: double ... 7 more fields]\nanswer: org.apache.spark.sql.DataFrame = [tpep_pickup_datetime: string, total_amount: double ... 10 more fields]\nanswer2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timeslot: string]\n+-------------------------+\n|timeslot                 |\n+-------------------------+\n|04:30:23 PM - 05:30:23 PM|\n+-------------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1606735374776_-440759595","id":"20201130-112254_1360274136","dateCreated":"2020-11-30T11:22:54+0000","dateStarted":"2020-12-01T02:15:43+0000","dateFinished":"2020-12-01T02:15:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5651"},{"user":"anonymous","dateUpdated":"2020-11-30T16:30:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606735494232_-1705075129","id":"20201130-112454_1030368759","dateCreated":"2020-11-30T11:24:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5652"}],"name":"ProjectFive","id":"2FR5JCN41","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}